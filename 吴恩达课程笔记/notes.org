* week1 homeworks
1. D
2. AC -> ACD
3. ABD
4. A -> B
5. C
6. B
7. B
8. AC
9. B
10. AB
* week2 notes
** Var
    x
    w
    b
    $z=w^{T}x+b$
    $y^{hat} = sigmoid(w^{T}x+b)=sigmoid(z)=\frac{1}{1+e^{-z}}$
    possibility should between 0~1 -> sigmoid

** cost function
    loss function: $L(y^{hat},y)=-(y*logy^{hat}+(1-y)log(1-y^{hat}))$
    cost function: $J(w,b)=\frac{1}{m}\sum_{i=1}^{m}L(y^{hat(i)},y^{i})=-\frac{1}{m}\sum_{i=1}^{m}(y*logy^{hat}+(1-y)log(1-y^{hat}))$

** Gradient Descent
    find w,b that minimize J
    repeat {
    w = w - $\alpha \frac{dJ(w,b)}{dw}$ = w - $\alpha dw$
    b = b - $\alpha \frac{dJ(w,b)}{db}$ = b - $\alpha db$
    }

** Forward and Backward
    J(a,b,c) = 3(a+bc)
    u = bc
    v = a + u
    J = 3v
    - Computation Graph
      a              ->    v = a + u -> J = 3v
      
      b  ->   u = bc ->
       
      c  ->
    Forward -> J
    Backward -> derivatives
    dFinalOutputVar/dvar = dvar

** Logistic Regression Gradient descent
*** Logistic regression recap
    $z=w^{T}x+b$
    $y^{hat} = sigmoid(w^{T}x+b)=sigmoid(z)=\frac{1}{1+e^{-z}}$
    $L(y^{hat},y)=-(y*logy^{hat}+(1-y)log(1-y^{hat}))$
    $x_1, w_1, x_2, w_2, b -> z = w_{1}x_{1}+w_{2}x_{2}+b -> y^{hat} = sigmoid(z) -> L(a,y)$
    compute $dw_1, dw_2, db$
    - do 
      $w_1 = w_1 - \alpha dw_1$
      $w_1 = w_2 - \alpha dw_2$
      $b = b - \alpha db$

** Gradient descent on m examples
    $J(w,b)=\frac{1}{m}\sum_{i=1}^{m}L(y^{hat(i)},y^{i})=-\frac{1}{m}\sum_{i=1}^{m}(y*logy^{hat}+(1-y)log(1-y^{hat}))$
    $a^{(i)}=y^{hat(i)}=sigmoid(z^{(i)})=sigmoid(w^{T}x^{(i)}+b)$
    - overall Alogrithm
      J = 0, dw1 = 0, dw2 = 0, db = 0
      - for i = 1 to m
	zi = wT * xi + b
	ai = sigmoid(zi)
	J += -(yi * log ai + (1 - yi) log(1 - ai))
	dzi = ai - yi
	dw1 += x_1i * dzi
	dw2 += x_2i * dzi
	db += dzi
      J/=m, dw1/=m, dw2/=m, db/=m
      w1 = w1 - alpha dw1
      w2 = w2 - alpha dw2
      b = b - alpha db

**  Vectorization of Logistic descent
   - overall Alogrithm with vectorization
     Z = np.dot(w.T, X) + b
     A = sigmoid(Z)
     dZ = A - Y
     dw = 1/m * X * dZ.T
     db = 1/m np.sum(dZ)
     w = w - alpha * dw
     b = b - alpha * db

* week2 homeworks
11. B
12. D
13. B
14. C
15. D
16. C
17. C
18. B
19. C -> A
20. B
Notation: *是元素乘法，dot是矩阵乘法
* week3 notes
** neural network
- compute
  - node at layer one
    $z_{i}^{[1]}=w_{i}^{[1]T}x+b_{i}^{[1]}$
    $a_{i}^{[1]}=sigmoid(z_{i}^{[1]})$
  then join all the vector together
  $$Z^{[1]}=\left[\begin{array}{1}
      z_{1}^{[1]} \\
      z_{2}^{[1]} \\
      z_{3}^{[1]} \\
      z_{4}^{[1]}
      \end{array}\right]$$
  $A=sigmoid(Z^{[1]})$
input layer
hidden layer: two parameters $w^{[1]}$ and $b^{[1]}$
output layer
** Justification for vectorized implementations
- join all the training data together
  Z[1](1) = w[1]x(1)+b[1]
  Z[1](2) = w[1]x(2)+b[1]
  Z[1](3) = w[1]x(3)+b[1]
           |
           |
           V
  W[1][x(1) x(2) x(3)] = W[1]Z[1]
** Activation functions
1. sigmoid
   a = $\sigma(z)$
2. tanh (always outperform sigmoid)
   a = $tanh(z)$
3. ReLu (much faster)
   a = max(0, Z)
4. Leaky ReLu (always outperform ReLu)
   many people don't use it
5. linear activation function (make the hidden layer no effect)
   doing machine learning on a regression problem
   output layer may use it
** deric
g(z) stand for activation function
1. sigmoid
   g'(z) = g(z)(1 - g(z))
   a = g(z) ->
   g'(z) = a(1 - a)
2. tanh
   g'(z) = 1 - g(z)^2
   g'(z) = 1 - a^2
3. ReLu and Leaky ReLu
   - ReLu
     g'(z) = 0 if z<0
     g'(z) = 1 if z>=0
   - Leaky ReLu
     g'(z) = 0.01 if z<0
     g'(z) = 1 if z>=0
** Gradient descent for neural networks
- parameters:
  w[1] (n[1], n[0])
  b[1] (n[1], 1)
  w[2] (n[2], n[1])
  b[2] (n[2], 1)
- cost function
  J(w[1] b[1] w[2] b[2]) = 1/m \sum L(y^hat, y)
- repeat
  compute predictions(y^hat(i), i 1->m)
  dw[1], db[1], dw[2], ....
  w[1] = w[1] - \alpha dw[1]
  b[1] = b[1] - \alpha db[1]
  ....
- forward propagation
  z[1] = w[1]X + b[1]
  A[1] = g[1](z[1])
  z[2] = w[2]A[2] + b[2]
  A[2] = g[2](z[2]) = sigmoid(z[2])
- backward propagation
  dz[2] = A[2] - Y
  dw[2] = 1/m dz[2]A[2]T
  db = 1/m np.sum(dz[2], axis = 1, keepdims = True)
  dz[1] = w[2]dz[2] * g[1]'(z[1])
  dw[1] = 1/m dz[1]XT
  db[1] = 1/m np.sum(dz[1], axis = 1, keepdims = True
** Random initialization
   w[1]=np.random.randn((2,2)) * 0.01
   b[1]=np.zeros((2,1))
   ....
* week3 homework
21. BDE -> BDEF
22. A
23. D
24. C
25. D
26. A
27. A -> B
28. D
29. BCEH
30. A -> C
因为有多组训练数据
