* week2 notes
** Var
    x
    w
    b
    $z=w^{T}x+b$
    $y^{hat} = sigmoid(w^{T}x+b)=sigmoid(z)=\frac{1}{1+e^{-z}}$
    possibility should between 0~1 -> sigmoid

** cost function
    loss function: $L(y^{hat},y)=-(y*logy^{hat}+(1-y)log(1-y^{hat}))$
    cost function: $J(w,b)=\frac{1}{m}\sum_{i=1}^{m}L(y^{hat(i)},y^{i})=-\frac{1}{m}\sum_{i=1}^{m}(y*logy^{hat}+(1-y)log(1-y^{hat}))$

** Gradient Descent
    find w,b that minimize J
    repeat {
    w = w - $\alpha \frac{dJ(w,b)}{dw}$ = w - $\alpha dw$
    b = b - $\alpha \frac{dJ(w,b)}{db}$ = b - $\alpha db$
    }

** Forward and Backward
    J(a,b,c) = 3(a+bc)
    u = bc
    v = a + u
    J = 3v
    - Computation Graph
      a              ->    v = a + u -> J = 3v
      
      b  ->   u = bc ->
       
      c  ->
    Forward -> J
    Backward -> derivatives
    dFinalOutputVar/dvar = dvar

** Logistic Regression Gradient descent
*** Logistic regression recap
    $z=w^{T}x+b$
    $y^{hat} = sigmoid(w^{T}x+b)=sigmoid(z)=\frac{1}{1+e^{-z}}$
    $L(y^{hat},y)=-(y*logy^{hat}+(1-y)log(1-y^{hat}))$
    $x_1, w_1, x_2, w_2, b -> z = w_{1}x_{1}+w_{2}x_{2}+b -> y^{hat} = sigmoid(z) -> L(a,y)$
    compute $dw_1, dw_2, db$
    - do 
      $w_1 = w_1 - \alpha dw_1$
      $w_1 = w_2 - \alpha dw_2$
      $b = b - \alpha db$

** Gradient descent on m examples
    $J(w,b)=\frac{1}{m}\sum_{i=1}^{m}L(y^{hat(i)},y^{i})=-\frac{1}{m}\sum_{i=1}^{m}(y*logy^{hat}+(1-y)log(1-y^{hat}))$
    $a^{(i)}=y^{hat(i)}=sigmoid(z^{(i)})=sigmoid(w^{T}x^{(i)}+b)$
    - overall Alogrithm
      J = 0, dw1 = 0, dw2 = 0, db = 0
      - for i = 1 to m
	zi = wT * xi + b
	ai = sigmoid(zi)
	J += -(yi * log ai + (1 - yi) log(1 - ai))
	dzi = ai - yi
	dw1 += x_1i * dzi
	dw2 += x_2i * dzi
	db += dzi
      J/=m, dw1/=m, dw2/=m, db/=m
      w1 = w1 - alpha dw1
      w2 = w2 - alpha dw2
      b = b - alpha db

**  Vectorization of Logistic descent
   - overall Alogrithm with vectorization
     Z = np.dot(w.T, X) + b
     A = sigmoid(Z)
     dZ = A - Y
     dw = 1/m * X * dZ.T
     db = 1/m np.sum(dZ)
     w = w - alpha * dw
     b = b - alpha * db

* week2 homeworks
11. B
12. D
13. B
14. C
15. D
16. C
17. C
18. B
19. C -> A
20. B
Notation: *是元素乘法，dot是矩阵乘法
